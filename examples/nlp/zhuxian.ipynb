{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fX5NUgo2pq3-"
   },
   "source": [
    "# 相关包调取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2230,
     "status": "ok",
     "timestamp": 1582342046734,
     "user": {
      "displayName": "ankang chu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAuIxhxsRb7j_rd4VVyK4bCBRTa_6h7_80neWOp=s64",
      "userId": "07574466021164400993"
     },
     "user_tz": -480
    },
    "id": "yCzY1inAl84e",
    "outputId": "d8acd2a0-8bf8-4a9b-d840-5bde20645cc2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4dfa5b6faf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import codecs\n",
    "import numpy as np\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNrIQS-2p_Tk"
   },
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30897,
     "status": "ok",
     "timestamp": 1582342075421,
     "user": {
      "displayName": "ankang chu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAuIxhxsRb7j_rd4VVyK4bCBRTa_6h7_80neWOp=s64",
      "userId": "07574466021164400993"
     },
     "user_tz": -480
    },
    "id": "jAJrdI6FqJC4",
    "outputId": "7c065487-1c77-49be-bb8e-57e51c697987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3126568 characters long\n"
     ]
    }
   ],
   "source": [
    "# Load the book as a string\n",
    "FILE_PATH = '/content/drive/My Drive/mine/dataset/zhuxian.txt'\n",
    "\n",
    "# Raw corpus of the book\n",
    "corpus_raw = u\"\"\n",
    "\n",
    "with codecs.open(FILE_PATH, 'r', 'utf-8') as book_file:\n",
    "    corpus_raw += book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66MJNsa8kH2u"
   },
   "outputs": [],
   "source": [
    "USE_SPLIT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rla7WdwM3XJo"
   },
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44508,
     "status": "ok",
     "timestamp": 1582342089049,
     "user": {
      "displayName": "ankang chu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAuIxhxsRb7j_rd4VVyK4bCBRTa_6h7_80neWOp=s64",
      "userId": "07574466021164400993"
     },
     "user_tz": -480
    },
    "id": "GG3thfcq3ayo",
    "outputId": "e490bf26-a9f6-4f38-bf69-d752dab750c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.838 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2044284, number of Chinese words in text: 38094\n"
     ]
    }
   ],
   "source": [
    "def create_lookup_tables(text, use_split=USE_SPLIT):\n",
    "  \"\"\"\n",
    "  Create lookup tables for vocab\n",
    "  :param text: The corpus text split into words\n",
    "  :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "  \"\"\"\n",
    "  words = list(jieba.cut(text))\n",
    "  vocab = set(words) if use_split else set(text)\n",
    "    \n",
    "  int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "  vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    \n",
    "  if use_split:\n",
    "      text_index = [vocab_to_int[word] for word in words]\n",
    "  else:\n",
    "      text_index = [vocab_to_int[word] for word in text]\n",
    "    \n",
    "  return vocab_to_int, int_to_vocab, text_index\n",
    "\n",
    "\n",
    "vocab_to_int, int_to_vocab, corpus_int = create_lookup_tables(corpus_raw)\n",
    "print(\"Vocabulary size: {}, number of Chinese words in text: {}\".format(len(corpus_int), len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rLo3Jb0kQGD"
   },
   "source": [
    "# 构造网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sfn1CWPk-SH"
   },
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-erK_gSQkUM4"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 400        # 将诛仙从头到尾训练多少遍\n",
    "batch_size = 512        # 每次训练feed的batch大小\n",
    "rnn_size = 128         # RNN Cell的Hidden Units的大小\n",
    "num_layers = 2         # RNN的层数\n",
    "keep_prob = 0.7         # Dropout保留率\n",
    "embed_dim = 128         # 词向量的维度，这个要和RNN Hidden Units的大小一致\n",
    "seq_length = 30         # Sequence的长度\n",
    "learning_rate = 0.001      # 学习率\n",
    "save_dir = '/content/drive/My Drive/mine/model/'       # 保存模型的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-X95jw2k_-X"
   },
   "source": [
    "## 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46820,
     "status": "ok",
     "timestamp": 1582342091383,
     "user": {
      "displayName": "ankang chu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAuIxhxsRb7j_rd4VVyK4bCBRTa_6h7_80neWOp=s64",
      "userId": "07574466021164400993"
     },
     "user_tz": -480
    },
    "id": "OEd-6gdolBSW",
    "outputId": "e779dd99-b3f4-4ff2-e866-a694260d2b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-37fc4881c016>:14: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-37fc4881c016>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From <ipython-input-8-37fc4881c016>:26: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Train Graph\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "  # Initialize input placeholders\n",
    "  input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "  targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "  lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "  # Calculate text attributes\n",
    "  vocab_size = len(int_to_vocab)\n",
    "  input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "  # Build the RNN cell\n",
    "  lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "  drop_cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "  cell = tf.contrib.rnn.MultiRNNCell([drop_cell] * num_layers)\n",
    "    \n",
    "  # Set the initial state\n",
    "  initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "  initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "  # Create word embedding as input to RNN\n",
    "  embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, embed_dim)\n",
    "    \n",
    "  # Build RNN\n",
    "  outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "  final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "  # Take RNN output and make logits\n",
    "  logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "  # Calculate the probability of generating each word\n",
    "  probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "  # Define loss function\n",
    "  cost = tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                       targets,\n",
    "                       tf.ones([input_text_shape[0], input_text_shape[1]]))\n",
    "    \n",
    "  # Learning rate optimizer\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "  # Gradient clipping to avoid exploding gradients\n",
    "  gradients = optimizer.compute_gradients(cost)\n",
    "  capped_gradients = [(tf.clip_by_value(grad, -1., 1.), value) for grad, value in gradients if grad is not None]\n",
    "  train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_YvLGBpnBUi"
   },
   "outputs": [],
   "source": [
    "## create data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcGIzY_EnGP1"
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "  \"\"\"\n",
    "  Return batches of input and target data\n",
    "  :param int_text: text with words replaced by their ids\n",
    "  :param batch_size: the size that each batch of data should be\n",
    "  :param seq_length: the length of each sequence\n",
    "  :return: batches of data as a numpy array\n",
    "  \"\"\"\n",
    "  words_per_batch = batch_size * seq_length\n",
    "  num_batches = len(int_text)//words_per_batch\n",
    "  int_text = int_text[:num_batches*words_per_batch]\n",
    "  y = np.array(int_text[1:] + [int_text[0]])\n",
    "  x = np.array(int_text)\n",
    "    \n",
    "  x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "  y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "  batch_data = list(zip(x_batches, y_batches))\n",
    "  return np.array(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4j9yvGKneqZ"
   },
   "source": [
    "## train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vvyzZJmXnZcs",
    "outputId": "f7834f10-8075-47d1-8818-946e3231ec66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Batches per Epoche : 133, Total Epochs : 400\n",
      "Epoch   1 Batch    1/133 train_loss = 10.548 time_per_batch = 2.202 time_elapsed = 10.080   time_remaining = 117431\n",
      "Model Trained and Saved\n",
      "Epoch   1 Batch  101/133 train_loss = 6.633 time_per_batch = 0.446 time_elapsed = 54.668   time_remaining = 23730\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch    1/133 train_loss = 6.636 time_per_batch = 0.449 time_elapsed = 69.636   time_remaining = 23893\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch  101/133 train_loss = 6.572 time_per_batch = 0.444 time_elapsed = 114.686   time_remaining = 23550\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch    1/133 train_loss = 6.595 time_per_batch = 0.448 time_elapsed = 129.708   time_remaining = 23784\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch  101/133 train_loss = 6.563 time_per_batch = 0.444 time_elapsed = 174.466   time_remaining = 23510\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch    1/133 train_loss = 6.585 time_per_batch = 0.434 time_elapsed = 189.961   time_remaining = 22955\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch  101/133 train_loss = 6.555 time_per_batch = 0.437 time_elapsed = 234.761   time_remaining = 23114\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch    1/133 train_loss = 6.579 time_per_batch = 0.440 time_elapsed = 249.822   time_remaining = 23252\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch  101/133 train_loss = 6.546 time_per_batch = 0.446 time_elapsed = 295.013   time_remaining = 23522\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch    1/133 train_loss = 6.573 time_per_batch = 0.450 time_elapsed = 310.105   time_remaining = 23680\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch  101/133 train_loss = 6.527 time_per_batch = 0.443 time_elapsed = 355.079   time_remaining = 23264\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch    1/133 train_loss = 6.543 time_per_batch = 0.439 time_elapsed = 370.638   time_remaining = 23071\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch  101/133 train_loss = 6.468 time_per_batch = 0.439 time_elapsed = 415.535   time_remaining = 23028\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch    1/133 train_loss = 6.481 time_per_batch = 0.440 time_elapsed = 430.736   time_remaining = 23080\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch  101/133 train_loss = 6.426 time_per_batch = 0.446 time_elapsed = 476.226   time_remaining = 23346\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch    1/133 train_loss = 6.447 time_per_batch = 0.452 time_elapsed = 491.469   time_remaining = 23621\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch  101/133 train_loss = 6.275 time_per_batch = 0.441 time_elapsed = 536.528   time_remaining = 22998\n",
      "Model Trained and Saved\n",
      "Epoch  10 Batch    1/133 train_loss = 6.214 time_per_batch = 0.452 time_elapsed = 552.142   time_remaining = 23547\n",
      "Model Trained and Saved\n",
      "Epoch  10 Batch  101/133 train_loss = 6.008 time_per_batch = 0.456 time_elapsed = 597.346   time_remaining = 23727\n",
      "Model Trained and Saved\n",
      "Epoch  11 Batch    1/133 train_loss = 6.005 time_per_batch = 0.454 time_elapsed = 612.608   time_remaining = 23602\n",
      "Model Trained and Saved\n",
      "Epoch  11 Batch  101/133 train_loss = 5.896 time_per_batch = 0.443 time_elapsed = 658.230   time_remaining = 23000\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch    1/133 train_loss = 5.907 time_per_batch = 0.442 time_elapsed = 673.404   time_remaining = 22917\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch  101/133 train_loss = 5.808 time_per_batch = 0.449 time_elapsed = 718.555   time_remaining = 23264\n",
      "Model Trained and Saved\n",
      "Epoch  13 Batch    1/133 train_loss = 5.808 time_per_batch = 0.446 time_elapsed = 734.209   time_remaining = 23067\n",
      "Model Trained and Saved\n",
      "Epoch  13 Batch  101/133 train_loss = 5.689 time_per_batch = 0.443 time_elapsed = 779.449   time_remaining = 22852\n",
      "Model Trained and Saved\n",
      "Epoch  14 Batch    1/133 train_loss = 5.696 time_per_batch = 0.455 time_elapsed = 794.709   time_remaining = 23491\n",
      "Model Trained and Saved\n",
      "Epoch  14 Batch  101/133 train_loss = 5.564 time_per_batch = 0.437 time_elapsed = 840.361   time_remaining = 22493\n",
      "Model Trained and Saved\n",
      "Epoch  15 Batch    1/133 train_loss = 5.572 time_per_batch = 0.447 time_elapsed = 855.652   time_remaining = 22984\n",
      "Model Trained and Saved\n",
      "Epoch  15 Batch  101/133 train_loss = 5.452 time_per_batch = 0.442 time_elapsed = 900.922   time_remaining = 22727\n",
      "Model Trained and Saved\n",
      "Epoch  16 Batch    1/133 train_loss = 5.472 time_per_batch = 0.445 time_elapsed = 916.594   time_remaining = 22871\n",
      "Model Trained and Saved\n",
      "Epoch  16 Batch  101/133 train_loss = 5.364 time_per_batch = 0.444 time_elapsed = 961.979   time_remaining = 22730\n",
      "Model Trained and Saved\n",
      "Epoch  17 Batch    1/133 train_loss = 5.391 time_per_batch = 0.451 time_elapsed = 977.284   time_remaining = 23110\n",
      "Model Trained and Saved\n",
      "Epoch  17 Batch  101/133 train_loss = 5.277 time_per_batch = 0.452 time_elapsed = 1023.022   time_remaining = 23085\n",
      "Model Trained and Saved\n",
      "Epoch  18 Batch    1/133 train_loss = 5.303 time_per_batch = 0.447 time_elapsed = 1038.332   time_remaining = 22848\n",
      "Model Trained and Saved\n",
      "Epoch  18 Batch  101/133 train_loss = 5.182 time_per_batch = 0.444 time_elapsed = 1083.778   time_remaining = 22614\n",
      "Model Trained and Saved\n",
      "Epoch  19 Batch    1/133 train_loss = 5.213 time_per_batch = 0.456 time_elapsed = 1099.538   time_remaining = 23212\n",
      "Model Trained and Saved\n",
      "Epoch  19 Batch  101/133 train_loss = 5.093 time_per_batch = 0.450 time_elapsed = 1144.803   time_remaining = 22889\n",
      "Model Trained and Saved\n",
      "Epoch  20 Batch    1/133 train_loss = 5.132 time_per_batch = 0.455 time_elapsed = 1160.274   time_remaining = 23133\n",
      "Model Trained and Saved\n",
      "Epoch  20 Batch  101/133 train_loss = 5.021 time_per_batch = 0.447 time_elapsed = 1205.978   time_remaining = 22660\n",
      "Model Trained and Saved\n",
      "Epoch  21 Batch    1/133 train_loss = 5.053 time_per_batch = 0.448 time_elapsed = 1221.375   time_remaining = 22704\n",
      "Model Trained and Saved\n",
      "Epoch  21 Batch  101/133 train_loss = 4.942 time_per_batch = 0.443 time_elapsed = 1266.787   time_remaining = 22413\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch    1/133 train_loss = 4.982 time_per_batch = 0.451 time_elapsed = 1282.578   time_remaining = 22818\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch  101/133 train_loss = 4.879 time_per_batch = 0.444 time_elapsed = 1327.867   time_remaining = 22391\n",
      "Model Trained and Saved\n",
      "Epoch  23 Batch    1/133 train_loss = 4.920 time_per_batch = 0.441 time_elapsed = 1343.638   time_remaining = 22206\n",
      "Model Trained and Saved\n",
      "Epoch  23 Batch  101/133 train_loss = 4.822 time_per_batch = 0.442 time_elapsed = 1389.016   time_remaining = 22253\n",
      "Model Trained and Saved\n",
      "Epoch  24 Batch    1/133 train_loss = 4.862 time_per_batch = 0.435 time_elapsed = 1404.397   time_remaining = 21892\n",
      "Model Trained and Saved\n",
      "Epoch  24 Batch  101/133 train_loss = 4.766 time_per_batch = 0.445 time_elapsed = 1450.151   time_remaining = 22331\n",
      "Model Trained and Saved\n",
      "Epoch  25 Batch    1/133 train_loss = 4.812 time_per_batch = 0.441 time_elapsed = 1465.480   time_remaining = 22130\n",
      "Model Trained and Saved\n",
      "Epoch  25 Batch  101/133 train_loss = 4.719 time_per_batch = 0.453 time_elapsed = 1510.939   time_remaining = 22675\n",
      "Model Trained and Saved\n",
      "Epoch  26 Batch    1/133 train_loss = 4.757 time_per_batch = 0.449 time_elapsed = 1526.754   time_remaining = 22441\n",
      "Model Trained and Saved\n",
      "Epoch  26 Batch  101/133 train_loss = 4.663 time_per_batch = 0.456 time_elapsed = 1572.228   time_remaining = 22739\n",
      "Model Trained and Saved\n",
      "Epoch  27 Batch    1/133 train_loss = 4.713 time_per_batch = 0.450 time_elapsed = 1587.626   time_remaining = 22462\n",
      "Model Trained and Saved\n",
      "Epoch  27 Batch  101/133 train_loss = 4.629 time_per_batch = 0.441 time_elapsed = 1633.089   time_remaining = 21941\n",
      "Model Trained and Saved\n",
      "Epoch  28 Batch    1/133 train_loss = 4.669 time_per_batch = 0.448 time_elapsed = 1648.517   time_remaining = 22285\n",
      "Model Trained and Saved\n",
      "Epoch  28 Batch  101/133 train_loss = 4.590 time_per_batch = 0.444 time_elapsed = 1694.358   time_remaining = 22043\n",
      "Model Trained and Saved\n",
      "Epoch  29 Batch    1/133 train_loss = 4.622 time_per_batch = 0.440 time_elapsed = 1709.760   time_remaining = 21831\n",
      "Model Trained and Saved\n",
      "Epoch  29 Batch  101/133 train_loss = 4.540 time_per_batch = 0.443 time_elapsed = 1755.214   time_remaining = 21909\n",
      "Model Trained and Saved\n",
      "Epoch  30 Batch    1/133 train_loss = 4.589 time_per_batch = 0.454 time_elapsed = 1771.045   time_remaining = 22456\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(corpus_int, batch_size, seq_length)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Num Batches per Epoche : {}, Total Epochs : {}\".format(num_batches, num_epochs))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for epoch in range(num_epochs):\n",
    "    state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "    for batch_index, (x, y) in enumerate(batches):\n",
    "      batch_start_time = time.time()\n",
    "      feed_dict = {input_text: x,\n",
    "              targets: y,\n",
    "              initial_state: state,\n",
    "              lr: learning_rate}\n",
    "      train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "      \n",
    "      if batch_index % 100 == 0:\n",
    "          time_elapsed   = time.time() - start_time\n",
    "          time_per_batch = time.time() - batch_start_time\n",
    "          num_batches_remaining = (num_epochs - epoch) * num_batches + num_batches - batch_index \n",
    "          print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f} time_per_batch = {:.3f} time_elapsed = {:.3f}   time_remaining = {:.0f}'.format(\n",
    "                    epoch + 1,\n",
    "                    batch_index + 1,\n",
    "                    num_batches,\n",
    "                    train_loss,\n",
    "                    time_per_batch,\n",
    "                    time_elapsed,\n",
    "                    num_batches_remaining * time_per_batch))\n",
    "          \n",
    "          # save model every 100 batches\n",
    "          saver = tf.train.Saver()\n",
    "          saver.save(sess, save_dir)\n",
    "          print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcjibo3hWg2S"
   },
   "source": [
    "## 文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8agigJvWa-O"
   },
   "outputs": [],
   "source": [
    "gen_length = 1000\n",
    "prime_words = '一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = list(jieba.cut(prime_words)) if USE_SPLIT else prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        # Get predict word\n",
    "        word_probs = probabilities[0][dyn_seq_length-1]\n",
    "        pred_word = pick_word(word_probs, int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ''.join(gen_sentences)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhp+NqjcBZvMtc+fhAOL3V",
   "collapsed_sections": [],
   "name": "zhuxian.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
